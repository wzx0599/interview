# MySQL45讲

## 1.一条sql查询语句的执行

<img src="https://static001.geekbang.org/resource/image/0d/d9/0d2070e8f84c4801adbfa03bda1f98d9.png" alt="img" style="zoom:33%;" />

:star:**连接器**：连接器负责跟客户端建立连接、获取权限、维持和管理连接

- 连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的**TCP握手**后，连接器就要开始认证身份
- 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时
- 数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个
- 全部使用长连接后，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启——两个解决方案：1.定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连；2.如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态

:star:**查询缓存**：连接建立完成后，可以执行select语句

- 之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中，key是查询的语句，value是查询的结果
- 不建议使用缓存——查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非业务就是一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。
- MySQL 8.0版本直接将查询缓存的整块功能删掉了

:star:**分析器**：词法分析+语法分析，一般语法错误会提示第一个出现错误的位置

:star:**优化器**：在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序

:star:**执行器**：先进行权限查询

- 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口

## 2.一条sql更新语句的执行

在一个表上有更新的时候，跟这个表有关的查询缓存会失效

与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和  binlog （归档日志）

:star:**redo log**：如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高

- WAL（Write-Ahead Logging），它的关键点就是先写日志，再写磁盘
- 当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。InnoDB的redo log是固定大小的
- 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**

:star:**binlog**： redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。

- redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用
- redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
- redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
- <img src="https://static001.geekbang.org/resource/image/2e/be/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png" alt="img" style="zoom:50%;" />
- redo log的两阶段提交保证崩溃恢复时库状态的一致性

## 3.事务隔离

在MySQL中，事务支持是在引擎层实现的

ACID：原子性、一致性、隔离性、持久性

:star:**隔离性和隔离级别**：

- 读未提交（read uncommitted）
- 读提交（read committed）
- 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的
- 串行化（serializable ）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

:point_right: 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

:star:**事务隔离的实现**：

<img src="https://static001.geekbang.org/resource/image/d9/ee/d9c313809e5ac148fc39feff532f0fee.png" alt="img" style="zoom:50%;" />

- 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作

- 在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的**多版本并发控制（MVCC）**。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。
- *回滚日志什么时候删除*？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除，即系统里没有比这个回滚日志更早的read-view的时候
- *为什么尽量不要使用长事务*？长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库，可以在information_schema库的innodb_trx这个表中查询长事务

## 4.索引（上）

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样，在MySQL中，索引是在存储引擎层实现的

:star:**提高读写效率常用的数据结构**

- 哈希表：增加新的记录时速度会很快，缺点是，因为不是有序的，所以哈希索引做**区间查询**的速度是很慢的，***哈希表这种结构适用于只有等值查询的场景***
- 有序数组：***有序数组在等值查询和范围查询场景中的性能就都非常优秀***，缺点是更新数据的时候就麻烦，则***有序数组索引只适用于静态存储引擎***
- 二叉查找树：每个节点的左儿子小于父节点，父节点又小于右儿子，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。

:star:**InnoDB的索引模型​**

- 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表，InnoDB使用了B+树索引模型。
- 索引类型分为主键索引和非主键索引：
  - 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）
  - 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）
- 主键索引与普通索引的区别：主键查询方式，则只需要搜索主键这棵B+树，而普通索引查询方式，则需要先搜索索引树，得到主键，再到主键索引树搜索一次。这个过程称为回表。

:star:**索引维护**

B+树为了维护索引有序性，在插入新值的时候需要做必要的维护

自增主键的插入数据模式，每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂，业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。***主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小***

## 5.索引（下）

:star:**覆盖索引**

如果执行的语句是select ID(主键) from T where k（索引） between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引

***覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段***

:star:**​前缀索引——最左前缀原则**

- 在建立联合索引的时候，如何安排索引内的字段顺序？
  - 如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的
  - 空间原则：比如市民表的情况，name字段是比age字段大的 ，那就建议创建一个（name,age)的联合索引和一个(age)的单字段索引

:star:**索引下推**

在最左前缀下如果还有其他条件限制：

- 在MySQL 5.6之前，只能从联合索引树找到的主键记录开始一个个回表。到主键索引上找出数据行，再对比字段值
- MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数

<img src="https://static001.geekbang.org/resource/image/b3/ac/b32aa8b1f75611e0759e52f5915539ac.jpg" alt="img" style="zoom: 30%;" /><img src="https://static001.geekbang.org/resource/image/76/1b/76e385f3df5a694cc4238c7b65acfe1b.jpg" alt="img" style="zoom: 28%;" />

## 6.全局锁和表锁

数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构

**MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类**

**:star:全局锁**

全局锁就是对整个数据库实例加锁

应用场景：做全库逻辑备份（全局读锁 FTWRL）

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆

- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。

官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。**single-transaction方法只适用于所有的表使用事务引擎的库**

***既然要全库只读，为什么不使用set global readonly=true的方式呢***？

- 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。如果用户有超级权限的话 readonly 是失效的
- 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高

**:star:表级锁**

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)

表锁一般是在数据库引擎不支持行锁的时候才会被用到的

- **表锁的语法是 lock tables … read/write。**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

- **MDL（metadata lock)**：MDL不需要显式使用，在访问一个表的时候会被自动加上，MDL的作用是，保证读写的正确性。当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁
  - 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
  - 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

<img src="https://static001.geekbang.org/resource/image/7c/ce/7cf6a3bf90d72d1f0fc156ececdfb0ce.jpg" alt="img" style="zoom:50%;" />

**如何安全地给小表加字段？**

事务不提交，就会一直占着MDL锁。在MySQL的information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。

kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程

## 7.行锁

MySQL的行锁是在引擎层由各个引擎自己实现的，并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度

行锁就是针对数据表中行记录的锁

**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议**。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放，这就最大程度地减少了事务之间的锁等待，提升了并发度

**:star:死锁和死锁检测**

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁

出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout（默认50s）来设置。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。——每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待（会出现热点行更新导致的性能问题，即很多线程等待同一个锁，检查到最后发现不是死锁）
- ***死锁检测要耗费大量的CPU资源***
  - 确保业务一定不会出现死锁，可以临时把死锁检测关掉
  - 控制并发度：这个并发控制要做在数据库服务端，基本思路就是，对于相同行的更新，在进入引擎之前排队
  - 通过将一行改成逻辑上的多行来减少锁冲突，将一行的值分散到多行上

## 8.事务的隔离和锁

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句（第一个快照读语句），事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令

在MySQL里，有两个“视图”的概念：

- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view ... ，而它的查询方法与表一样。
- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

**“快照”在MVCC里是怎么工作的？**

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。

而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id（通过undo log 回滚日志计算得到）

<img src="https://static001.geekbang.org/resource/image/88/5e/882114aaf55861832b4270d44507695e.png" alt="img" style="zoom:33%;" /> InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。

数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）

对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
   a. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见；
   b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）**

除了update语句外，select语句如果加锁，也是当前读。

所以，如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或 for update，也都是当前读。加了之后的这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）

***事务的可重复读的能力是怎么实现的？***

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而**读提交**的逻辑和**可重复读**的逻辑类似，它们最主要的区别是：

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

表结构不支持“可重复读”，这是因为表结构没有对应的行数据，也没有row trx_id，因此只能遵循当前读的逻辑

## 9.普通索引和唯一索引如何选择

普通索引+change buffer，在更新记录的时候若没在内存则先写入change buffer后面再读进内存时写入内存（merge）或定时写进磁盘；而唯一索引由于要判断唯一性所以要将记录读到内存中，**redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗**。

## 10.MySQL为什么有时候会选错索引

当表有多个索引时，优化器会根据需要扫描的行统计和其他的逻辑判断选择索引，而由于统计的不准确会导致优化器选错索引，造成执行效率降低

索引的区分度：索引的不同值越多表示区分度越高

在事务提交的时候，我们把change buffer的操作也记录到redo log里

## 11.怎么给字符串字段加索引

- **使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本**

前缀索引对覆盖索引的影响，满足前缀索引后还需要回表查询

改善方法：

- **使用倒序存储**

- **使用hash字段。**可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。

  字符串加索引的方法：

1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描

## 12.为什么我的MySQL会“抖”一下

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”**

平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）

## 13.为什么表数据删掉一半，表文件大小不变

delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过delete命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”

经过大量增删改的表，都是可能是存在空洞的，重建表可以收缩表空间

## 14.为什么count（*）那么慢

**count(*)的实现方式**：不同的MySQL引擎中，count(*)有不同的实现方式，这里指的是没有where条件的count

- MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；
- InnoDB引擎，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

**为什么InnoDB不跟MyISAM一样，也把数字存起来呢？**

由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。每一行记录都要判断自己是否对这个会话可见，因此对于count()请求来说，InnoDB只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。

*如果你用过show table status 命令的话，就会发现这个命令的输出结果里面也有一个TABLE_ROWS用于显示这个表当前有多少行，这个命令执行挺快的，那这个TABLE_ROWS能代替count(*)吗？

就像索引统计的值是通过采样来估算的，实际上，TABLE_ROWS就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到40%到50%。所以，show table status命令显示的行数也不能直接使用。

如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？答案是，我们只能自己计数。计数方法有哪些：

- 用缓存系统保存计数：内存记录一个行数值Redis，表被插入一行就加一删除一行就减一：缺点是1.易丢失，可在每次重启后执行count并给Redis重新赋值2.即使Redis正常工作，这个值也是逻辑上不精确的
- 在数据库保存计数：InnoDB是支持崩溃恢复不丢数据的、InnoDB的事务可以保证逻辑上是一致的

**不同的count用法**：

count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的**参数**不是NULL，累计值就加1，否则不加。最后返回累计值。

分析性能差别的时候，可以记住这么几个原则：

1. server层要什么就给什么；
2. InnoDB只给必要的值；
3. 现在的优化器只优化了count(*)的语义为“取行数”，其他“显而易见”的优化并没有做。

- **对于count(主键id)来说**，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加

- **对于count(1)来说**，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。单看这两个用法的差别的话，你能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。

- **对于count(字段)来说**：

  1. 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加

  2. 如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。

     **但是count(\*)是例外**，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加

按照效率排序的话，count(字段)<count(主键id)<count(1)≈count(\*)，所以我建议你，尽量使用count(\*)。

本期问题：

*由于事务可以保证中间结果不被别的事务读到，因此修改计数值和插入新记录的顺序是不影响逻辑结果的。但是，从并发系统性能的角度考虑，你觉得在这个事务序列里，应该先插入操作记录，还是应该先更新计数表呢？*

因为更新计数表涉及到行锁的竞争，先插入再更新能最大程度地减少了事务之间的锁等待，提升了并发度。

## 15.答疑文章（主要是关于redo log 和 binlog的相关问题）

## 16. order by是如何工作的

- 全字段排序

<img src="https://static001.geekbang.org/resource/image/6c/72/6c821828cddf46670f9d56e126e3e772.jpg" alt="img" style="zoom:50%;" />该算法只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。该算法存在的问题是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差

- rowid排序

<img src="https://static001.geekbang.org/resource/image/dc/6d/dc92b67721171206a302eb679c83e86d.jpg" alt="img" style="zoom:50%;" />

rowid排序多访问了一次表t的主键索引

MySQL的一个设计思想：**如果内存够，就要多利用内存，尽量减少磁盘访问。**对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

- 联合索引方法使查找字段本身有序

## 18.为什么这些SQL语句逻辑相同，性能却差异巨大

1. **条件字段函数操作**——如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**
2. **隐式类型转换**——有数据类型转换，就需要走全索引扫描
   - 在MySQL中，字符串和数字做比较的话，是将字符串转换成数字。
3. **隐式字符编码转换**
   - 两个表的字符集不同，一个是utf8，一个是utf8mb4，做表连接查询的时候用不上关联字段的索引。utf8mb4是utf8的超集。字符集不同只是条件之一，**连接过程中要求在被驱动表的索引字段上加函数操作**，是直接导致对被驱动表做全表扫描的原因

## 19.为什么只查一行的语句，也执行这么慢

1. **查询长时间不返回**

   一般碰到这种情况的话，大概率是表t被锁住了。接下来分析原因的时候，一般都是首先执行一下**show processlist**命令，看看当前语句处于什么状态。

   - **等MDL锁**——出现**这个状态表示的是，现在有一个线程正在表t上请求或者持有MDL写锁，把select语句堵住了。**通过查询sys.schema_table_lock_waits这张表，我们就可以直接找出造成阻塞的process id

     <img src="https://static001.geekbang.org/resource/image/50/28/5008d7e9e22be88a9c80916df4f4b328.png" alt="img" style="zoom:50%;" />

   - **等flush**
   - **等行锁**

2. **查询慢**

   select * from t where id = 1;(一致性读/可重复读)和select * from t where id = 1 lock in share mode;（读已提交）

   如果在该两句执行之前已经有另一个事务更改了很多次id=1的数据，那么前面的语句会执行的很慢，因为要不断从undo log里回溯该事务创建时的值；而后面的语句执行得较快

## 20.幻读是什么，幻读有什么问题

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

1. 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
2. 上面session B的修改结果，被session A之后的select语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。

查询加了for update，是当前读（参考前文，是可重复读更新数据时的度方法，区别读已提交）。当前读的规则，就是要能读到所有已经提交的记录的最新值

**幻读有什么问题？**

**1.语义上的**

**2.数据一致性的问题**

**即使把所有的记录都加上锁，还是阻止不了新插入的记录**

**解决幻读？**

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也就是间隙锁(Gap Lock)，间隙锁，锁的就是两个值之间的空隙

跟行锁有冲突关系的是“另外一个行锁”。但是间隙锁不一样，**跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。**间隙锁之间都不存在冲突关系

**幻读引入的问题**

间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的

本节的分析前提是在可重复读级别，间隙锁是在可重复读隔离级别下才会生效的，如果把隔离级别设置为读提交的话，就没有间隙锁了

## 21.为什么只改一行的语句，锁这么多

接下来的描述，若没有特殊说明，默认是可重复读隔离级别。

**加锁规则**

1. 原则1：加锁的基本单位是next-key lock（行锁＋间隙锁）。next-key lock是前开后闭区间。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
5. 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止

**案例一：等值查询间隙锁**

**案例二：非唯一索引等值锁**

- 锁是加在索引上的，如果要用lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段

**案例三：主键索引范围锁**

**案例四：非唯一索引范围锁**

**案例五：唯一索引范围锁bug**

**案例六：非唯一索引上存在"等值"的例子**

**案例七：limit 语句加锁**

- **在删除数据的时候尽量加limit**。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围

**案例八：一个死锁的例子**

- 在分析加锁规则的时候可以用next-key lock来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。

读提交隔离级别下，锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因

## 22. MySQL有哪些“饮鸩止渴”提高性能的方法

## 23. MySQL是怎么保证数据不丢的

**binlog的写入机制**——事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中

<img src="https://static001.geekbang.org/resource/image/9e/3e/9ed86644d5f39efb0efec595abb92e3e.png" alt="img" style="zoom:50%;" />

每个线程有自己binlog cache，但是共用同一份binlog文件。

- 图中的write，指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。
- 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。

write 和fsync的时机，是由参数sync_binlog控制的：

1. sync_binlog=0的时候，表示每次提交事务都只write，不fsync；
2. sync_binlog=1的时候，表示每次提交事务都会执行fsync；
3. sync_binlog=N(N>1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。

因此，在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其设置为100~1000中的某个数值。

但是，将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。

**redo log的写入机制**

<img src="https://static001.geekbang.org/resource/image/9d/d4/9d057f61d3962407f413deebc80526d4.png" alt="img" style="zoom:50%;" />

redo log可能存在的三种状态

1. 存在redo log buffer中，物理上是在MySQL进程内存中，就是图中的红色部分；
2. 写到磁盘(write)，但是没有持久化（fsync)，物理上是在文件系统的page cache里面，也就是图中的黄色部分；
3. 持久化到磁盘，对应的是hard disk，也就是图中的绿色部分。

为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值：

1. 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中;
2. 设置为1的时候，表示每次事务提交时都将redo log直接持久化到磁盘；
3. 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。

InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘。

还有两种场景会让一个没有提交的事务的redo log写入到磁盘中。

1. **一种是，redo log buffer占用的空间即将达到 innodb_log_buffer_size一半的时候，后台线程会主动写盘。**注意，由于这个事务并没有提交，所以这个写盘动作只是write，而没有调用fsync，也就是只留在了文件系统的page cache。
2. **另一种是，并行的事务提交的时候，顺带将这个事务的redo log buffer持久化到磁盘。**假设一个事务A执行到一半，已经写了一些redo log到buffer中，这时候有另外一个线程的事务B提交，如果innodb_flush_log_at_trx_commit设置的是1，那么按照这个参数的逻辑，事务B要把redo log buffer里的日志全部持久化到磁盘。这时候，就会带上事务A在redo log buffer里的日志一起持久化到磁盘。

通常我们说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是redo log（prepare 阶段），一次是binlog。

**组提交（group commit）机制**

日志逻辑序列号（log sequence number，LSN），LSN是单调递增的，用来对应redo log的一个个写入点，每次写入长度为length的redo log， LSN的值就会加上length。LSN也会写到InnoDB的数据页中，来确保数据页不会被多次执行重复的redo log。

WAL机制主要得益于两个方面：

1. redo log 和 binlog都是顺序写，磁盘的顺序写比随机写速度要快；
2. 组提交机制，可以大幅度降低磁盘的IOPS消耗

## 24. MySQL是怎么保证主备一致的

binlog可以用来归档，也可以用来做主备同步

**MySQL主备的基本原理**

<img src="https://static001.geekbang.org/resource/image/fd/10/fd75a2b37ae6ca709b7f16fe060c2c10.png" alt="img" style="zoom:50%;" />

在状态1中，客户端的读写都直接访问节点A，而节点B是A的备库，只是将A的更新都同步过来，到本地执行。这样可以保持节点B和A的数据是相同的。当需要切换的时候，就切成状态2。这时候客户端读写访问的都是节点B，而节点A是B的备库。

readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。

<img src="https://static001.geekbang.org/resource/image/a6/a3/a66c154c1bc51e071dd2cc8c1d6ca6a3.png" alt="img" style="zoom:50%;" />

主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写binlog。

备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。
2. 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。
3. 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。
4. 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。
5. sql_thread读取中转日志，解析出日志里的命令，并执行。

**binlog的三种格式对比**

statement、row、mixed

当binlog_format=statement时，binlog里面记录的就是SQL语句的原文，如果主备索引过程中采用了不同的索引就会导致主备不一致（如删除时使用limit语句）

***为什么会有mixed格式的binlog？***

- 因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。
- 但row格式的缺点是，很占空间。比如你用一个delete语句删掉10万行数据，用statement的话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果用row格式的binlog，就要把这10万条记录都写到binlog中。这样做，不仅会占用更大的空间，同时写binlog也要耗费IO资源，影响执行速度。
- 所以，MySQL就取了个折中方案，也就是有了mixed格式的binlog。mixed格式的意思是，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式，否则就用statement格式。

即使执行的是delete语句，row格式的binlog也会把被删掉的行的整行信息保存起来。所以，如果在执行完一条delete语句以后，发现删错数据了，可以直接把binlog中记录的delete语句转成insert，把被错删的数据插入回去就可以恢复了。——row格式便于恢复数据

binlog的特性确保了在备库执行相同的binlog，可以得到与主库相同的状态

两个都为主库存在的问题：循环复制（业务逻辑在节点A上更新了一条语句，然后再把生成的binlog 发给节点B，节点B执行完这条更新语句后也会生成binlog。（我建议你把参数log_slave_updates设置为on，表示备库执行relay log后生成binlog，如果节点A同时是节点B的备库，相当于又把节点B新生成的binlog拿过来执行了一次，然后节点A和B间，会不断地循环执行这个更新语句，也就是循环复制了））

MySQL在binlog中记录了这个命令第一次执行时所在实例的server id。因此，可以用下面的逻辑，来解决两个节点间的循环复制的问题：

1. 规定两个库的server id必须不同，如果相同，则它们之间不能设定为主备关系；
2. 一个备库接到binlog并在重放的过程中，生成与原binlog的server id相同的新的binlog；
3. 每个库在收到从自己的主库发过来的日志后，先判断server id，如果跟自己的相同，就丢弃

## 25.MySQL是怎么保证高可用的

- **主备延迟**

主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电

“**同步延迟**”。与数据同步有关的时间点主要包括以下三个：

1. 主库A执行完成一个事务，写入binlog，我们把这个时刻记为T1;
2. 之后传给备库B，我们把备库B接收完这个binlog的时刻记为T2;
3. 备库B执行完成这个事务，我们把这个时刻记为T3。

**主备延迟**，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是T3-T1

在网络正常的时候，日志从主库传给备库所需的时间是很短的，即T2-T1的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产binlog的速度要慢。

- **主备延迟的来源**

👉有些部署条件下，备库所在机器的性能要比主库所在的机器性能差，

​	做这种部署时，一般都会将备库设置为“非双1”的模式。当备库主机上的多个备库都在争抢资源的时候，就可能会导致主备延迟了。做了对称部署（主备库的机器一致）以后，还可能会有延迟。这是为什么呢？

👉备库的压力大

​	一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。备库上的查询耗费了大量的CPU资源，影响了同步速度，造成主备延迟。

一般可以这么处理：

1. 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。
2. 通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力

👉大事务

​	因为主库上必须等事务执行完成才会写入binlog，再传给备库。所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟。不要**一次性地用delete语句删除太多数据**。其实，这就是一个典型的大事务场景。**另一种典型的大事务场景，就是大表DDL**

👉备库的并行复制能力

- **主备切换的策略**

👉可靠性优先策略：等主备延迟比较小时开始切换，主备数据同步完成时切换备库的状态，变为主库，缺点是会有主备延迟这段时间系统处于不可用状态

<img src="https://static001.geekbang.org/resource/image/54/4a/54f4c7c31e6f0f807c2ab77f78c8844a.png" alt="img" style="zoom:50%;" />

👉可用性优先策略：直接将备库B切换为可读写状态，缺点是可能会出现主备数据不一致的情况，若采用row格式的binlog会在数据不一致时报错

<img src="https://static001.geekbang.org/resource/image/37/3a/3786bd6ad37faa34aca25bf1a1d8af3a.png" alt="img" style="zoom:50%;" />

## 26.备库为什么会延迟好几个小时

如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远都追不上主库的节奏

<img src="https://static001.geekbang.org/resource/image/1a/ef/1a85a3bac30a32438bfd8862e5a34eef.png" alt="img" style="zoom:50%;" />

主备的并行复制能力，要关注的是图中黑色的两个箭头。一个箭头代表了客户端写入主库，另一箭头代表的是备库上sql_thread执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图1所示，第一个箭头要明显粗于第二个箭头。

备库的多线程复制：（worker指一个线程）

1. 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个worker中。
2. 同一个事务不能被拆开，必须放到同一个worker中。

## 27.主库出问题了，从库怎么办

<img src="https://static001.geekbang.org/resource/image/aa/79/aadb3b956d1ffc13ac46515a7d619e79.png" alt="img" style="zoom:50%;" />

​	虚线箭头表示的是主备关系，也就是A和A’互为主备， 从库B、C、D指向的是主库A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担

**在一主多从架构下，主库故障后的主备切换问题。**

GTID的全称是Global Transaction Identifier，也就是全局事务ID，是一个事务在提交的时候生成的，是这个事务的唯一标识，它由两部分组成，格式是：

```
GTID=server_uuid:gno
```

- server_uuid是一个实例第一次启动时自动生成的，是一个全局唯一的值；

- gno是一个整数，初始值是1，每次提交事务的时候分配给这个事务，并加1。

  **基于GTID的主备切换**

## 28.读写分离的坑

## 29.如何判断一个数据库是否出问题了

## 30.答疑——用动态的观点看加锁

## 31.误删数据后怎么办

对和MySQL相关的误删数据，做下分类：

1. 使用delete语句误删数据行；
2. 使用drop table或者truncate table语句误删数据表；
3. 使用drop database语句误删数据库；
4. 使用rm命令误删整个MySQL实例。

**误删行**

delete语句可以用row格式的binlog恢复，使用truncate /drop table和drop database命令删除的数据binlog自动是statement格式

**误删库/表**

需要使用全量备份，加增量日志的方式

<img src="https://static001.geekbang.org/resource/image/2f/db/2fafd0b75286e0163f432f85428ff8db.png" alt="img" style="zoom:50%;" />

**预防误删库/表的方法**

第一条建议是，账号分离。这样做的目的是，避免写错命令。比如：

- 我们只给业务开发同学DML权限，而不给truncate/drop权限。而如果业务开发人员有DDL需求的话，也可以通过开发管理系统得到支持。
- 即使是DBA团队成员，日常也都规定只使用只读账号，必要的时候才使用有更新权限的账号

第二条建议是，制定操作规范。这样做的目的，是避免写错要删除的表名。比如：

- 在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。
- 改表名的时候，要求给表名加固定的后缀（比如加_to_be_deleted)，然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。

## 32.为什么还有kill不掉的语句

**收到kill以后，线程做什么？**

**当用户执行kill query thread_id_B时，MySQL里处理kill命令的线程做了两件事：**

1. 把session B的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)；
2. 给session B的执行线程发一个信号。

## 33.数据库内存机制

InnoDB内存的一个作用，是保存更新的结果，再配合redo log，就避免了随机写盘。内存的数据页是在Buffer Pool (BP)中管理的，在WAL里Buffer Pool 起到了加速更新的作用，Buffer Pool还有加速查询的作用。

InnoDB内存管理用的是最近最少使用 (Least Recently Used, LRU)算法，这个算法的核心就是淘汰最久未使用的数据。

InnoDB管理Buffer Pool的LRU算法，是用链表来实现的。最近使用的放在链表最前面，淘汰链表尾的节点

- InnoDB不能直接使用这个LRU算法。实际上，InnoDB对LRU算法做了改进。

在InnoDB实现上，按照5:3的比例把整个LRU链表分成了young区域和old区域，靠近链表头部的5/8是young区域，靠近链表尾部的3/8是old区域。

处于old区域的数据页，每次被访问的时候都要做下面这个判断：

- 若这个数据页在LRU链表中存在的时间超过了1秒，就把它移动到链表头部；
- 如果这个数据页在LRU链表中存在的时间短于1秒，位置保持不变。1秒这个时间，是由参数innodb_old_blocks_time控制的。其默认值是1000，单位毫秒。

这个策略，就是为了处理类似全表扫描的操作量身定制的。还是以扫描200G的历史数据表为例，我们看看改进后的LRU算法的操作逻辑：

1. 扫描过程中，需要新插入的数据页，都被放到old区域;
2. 一个数据页里面有多条记录，这个数据页会被多次访问到，但由于是顺序扫描，这个数据页第一次被访问和最后一次被访问的时间间隔不会超过1秒，因此还是会被保留在old区域；
3. 再继续扫描后续的数据，之前的这个数据页之后也不会再被访问到，于是始终没有机会移到链表头部（也就是young区域），很快就会被淘汰出去。

可以看到，这个策略最大的收益，就是在扫描这个大表的过程中，虽然也用到了Buffer Pool，但是对young区域完全没有影响，从而保证了Buffer Pool响应正常业务的查询命中率。

## 34.join

join语句的两种算法，分别是Index Nested-Loop Join(NLJ)和Block Nested-Loop Join(BNL)

1. 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好；
2. 如果使用join语句的话，需要让小表做驱动表。这个结论的前提是“可以使用被驱动表的索引”。

结论：可以用上被驱动表的索引是可以用join的，小表做驱动表：**在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。**

## 35.join语句的优化

**Multi-Range Read优化(MRR)**，这个优化的主要目的是尽量使用顺序读盘。

**因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。**

<img src="https://static001.geekbang.org/resource/image/d5/c7/d502fbaea7cac6f815c626b078da86c7.jpg" alt="img" style="zoom:50%;" />

**Batched Key Access**（BAK算法）实际是对NLJ算法的优化，使用MRR

**BNL的优化**——转化成BKA算法，优化的方向就是给被驱动表的关联字段加上索引

**BNL的被驱动表不好加索引时，采用临时表**

1. 把表t2中满足条件的数据放在临时表tmp_t中；
2. 为了让join使用BKA算法，给临时表tmp_t的字段b加上索引；
3. 让表t1和tmp_t做join操作。

## 36. 为什么临时表可以重名

- 内存表，指的是使用Memory引擎的表，建表语法是create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。
- 而临时表，可以使用各种引擎类型 。如果是使用InnoDB引擎或者MyISAM引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用Memory引擎。

临时表在使用上有以下几个特点：

1. 建表语法是create temporary table …。
2. 一个临时表只能被创建它的session访问，对其他线程不可见。所以，图中session A创建的临时表t，对于session B就是不可见的。
3. 临时表可以与普通表同名。
4. session A内有同名的临时表和普通表的时候，show create语句，以及增删改查语句访问的是临时表。
5. show tables命令不显示临时表

在执行

```sql
create temporary table temp_t(id int primary key)engine=innodb;
```

这个语句的时候，MySQL要给这个InnoDB表创建一个frm文件保存表结构定义，还要有地方保存表数据。

**这个frm文件放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程id}\_{线程id}\_序列号”**。你可以使用select @@tmpdir命令，来显示实例的临时文件目录。

## 37.内部临时表

1. 果对group by语句的结果没有排序要求，要在语句后面加 order by null；
2. 尽量让group by过程用上表的索引，确认方法是explain结果里没有Using temporary 和 Using filesort；
3. 如果group by需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大tmp_table_size参数，来避免用到磁盘临时表；
4. 如果数据量实在太大，使用SQL_BIG_RESULT这个提示，来告诉优化器直接使用排序算法得到group by的结果

## 38.Memory引擎

**内存表的数据组织结构**

Memory引擎的数据和索引是分开的

内存表的数据部分以数组的方式单独存放，而主键id索引里，存的是每个数据的位置

InnoDB和Memory引擎的数据组织方式是不同的：

- InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为**索引组织表**（Index Organizied Table）。
- 而Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为**堆组织表**（Heap Organizied Table）。

两个引擎的一些典型不同：

1. InnoDB表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；
2. 当数据文件有空洞的时候，InnoDB表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值；
3. 数据位置发生变化的时候，InnoDB表只需要修改主键索引，而内存表需要修改所有索引；
4. InnoDB表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。
5. InnoDB支持变长数据类型，不同记录的长度可能不同；内存表不支持Blob 和 Text字段，并且即使定义了varchar(N)，实际也当作char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。

**内存表的锁**

内存表不支持行锁，只支持表锁。一张表只要有更新，就会堵住其他所有在这个表上的读写操作，内存表的锁粒度问题，决定了它在处理并发事务的时候，性能也不会太好。

**数据持久性问题**

建议把普通内存表都用InnoDB表来代替，有一个例外，就是上节的内存临时表

1. 临时表不会被其他线程访问，没有并发性的问题；
2. 临时表重启后也是需要删除的，清空数据这个问题不存在；
3. 备库的临时表也不会影响主库的用户线程。

## 39. 自增主键为什么不是连续的

自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑

**自增值保存在哪儿？**

**表的结构定义存放在后缀名为.frm的文件中，但是并不会保存自增值。**

不同的引擎对于自增值的保存策略不同。

- MyISAM引擎的自增值保存在数据文件中。
- InnoDB引擎的自增值，其实是保存在了内存里，并且到了MySQL 8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是：
  - 在MySQL 5.7及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值max(id)，然后将max(id)+1作为这个表当前的自增值。﻿
    举例来说，如果一个表当前数据行里最大的id是10，AUTO_INCREMENT=11。这时候，我们删除id=10的行，AUTO_INCREMENT还是11。但如果马上重启实例，重启后这个表的AUTO_INCREMENT就会变成10。﻿
    也就是说，MySQL重启可能会修改一个表的AUTO_INCREMENT的值。
  - 在MySQL 8.0版本，将自增值的变更记录在了redo log中，重启的时候依靠redo log恢复重启之前的值。

**自增值修改机制**

- **唯一键冲突是导致自增主键id不连续的第一种原因。**

- **回滚也会产生类似的现象**

## 40.insert语句的锁

## 41.快速复制一张表

为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。

👉一种方法是，使用mysqldump命令将数据导出成一组INSERT语句。

👉直接将结果导出成.csv文件

👉导出+导入表空间的方式，实现物理拷贝表的功能